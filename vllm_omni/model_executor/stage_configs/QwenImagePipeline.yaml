# stage config for running qwen2.5-omni with architecture of OmniLLM.
stage_args:
  - stage_id: 0
    runtime:
      process: true            # Run this stage in a separate process
      devices: "0"            # Visible devices for this stage (CUDA_VISIBLE_DEVICES/torch.cuda.set_device)
      max_batch_size: 1
    engine_args:
      model_stage: text_encoder
      model_arch: Qwen2_5_VLForConditionalGeneration__
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.scheduler.OmniScheduler
      gpu_memory_utilization: 0.8
      enforce_eager: true  # need to discuss
      trust_remote_code: true
      engine_output_type: latent  # change the param name,such as pooling_output
      enable_prefix_caching: false
    final_output: false
  - stage_id: 1
    runtime:
      process: true
      devices: "1"
      max_batch_size: 1
    engine_args:
      model_stage: diffusion
      model_arch: QwenImagePipeline
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.scheduler.OmniScheduler
      gpu_memory_utilization: 0.8
      enforce_eager: true
      trust_remote_code: true
      engine_output_type: latent
    engine_input_source: [0]


# Top-level runtime config (concise): default windows and stage edges
runtime:
  enabled: true
  defaults:
    window_size: -1             # Simplified: trigger downstream only after full upstream completion
    max_inflight: 1             # Simplified: process serially within each stage
  edges:
    - from: 0                   # thinker â†’ talker: trigger only after receiving full input (-1)
      to: 1
      window_size: -1