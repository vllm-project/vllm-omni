# Stage config for running Fun-Audio-Chat-8B
#
# Fun-Audio-Chat supports two modes:
# 1. Speech-to-Text (S2T): Audio input → Text output
# 2. Speech-to-Speech (S2S): Audio input → Text + Speech tokens → Audio output (via CosyVoice3)
#
# This config demonstrates S2T mode. For S2S, add a second stage with CosyVoice3.
#
# Model: https://huggingface.co/FunAudioLLM/Fun-Audio-Chat-8B
# CosyVoice3: https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512

# Configuration for single-stage Speech-to-Text mode
# Tested on: 1x H100-80G GPU

stage_args:
  - stage_id: 0
    runtime:
      devices: "0"
      max_batch_size: 1
    engine_args:
      model_stage: main
      model_arch: FunAudioChatForConditionalGeneration
      worker_cls: vllm_omni.worker.gpu_ar_worker.GPUARWorker
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.85
      enforce_eager: true
      trust_remote_code: true # Required for custom model code
      engine_output_type: text
      enable_prefix_caching: false
      max_num_batched_tokens: 16384
      distributed_executor_backend: "mp"
    final_output: true
    final_output_type: text
    is_comprehension: true
    default_sampling_params:
      temperature: 0.7
      top_p: 0.9
      top_k: 50
      max_tokens: 2048
      seed: 42
      detokenize: true
      repetition_penalty: 1.05
# ============================================================================
# Speech-to-Speech Configuration (Work in Progress)
# ============================================================================
# For S2S mode, uncomment the following stage to add CosyVoice3:
#
#   - stage_id: 1
#     runtime:
#       devices: "0"
#       max_batch_size: 1
#     engine_args:
#       model_stage: cosyvoice
#       model_arch: FunAudioChatForConditionalGeneration
#       worker_cls: vllm_omni.worker.gpu_generation_worker.GPUGenerationWorker
#       scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
#       enforce_eager: true
#       trust_remote_code: true
#       enable_prefix_caching: false
#       engine_output_type: audio
#       gpu_memory_utilization: 0.1
#       distributed_executor_backend: "mp"
#       max_num_batched_tokens: 1000000
#       # Path to CosyVoice3 model
#       # model: path/to/Fun-CosyVoice3-0.5B-2512
#     engine_input_source: [0]
#     custom_process_input_func: vllm_omni.model_executor.stage_input_processors.fun_audio_chat.main2cosyvoice
#     final_output: true
#     final_output_type: audio
#     default_sampling_params:
#       temperature: 0.0
#       top_p: 1.0
#       top_k: -1
#       max_tokens: 65536
#       seed: 42
#       detokenize: true
