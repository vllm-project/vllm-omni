"""
vLLM-Omni: Multi-modality models inference and serving with
non-autoregressive structures.

This package extends vLLM beyond traditional text-based, autoregressive
generation to support multi-modality models with non-autoregressive
structures and non-textual outputs.

Architecture:
- ğŸŸ¡ Modified: vLLM components modified for multimodal support
- ğŸ”´ Added: New components for multimodal and non-autoregressive
  processing
"""

# vllm_omni/__init__.py
import vllm
from transformers import AutoConfig, Qwen2Config
from vllm.model_executor.models import ModelRegistry

from vllm_omni.model_executor.models.bagel.bagel import BagelForConditionalGeneration
from vllm_omni.model_executor.models.bagel.configuration_bagel import BagelConfig
from vllm_omni.model_executor.models.bagel.qwen2_bagel import Qwen2ForCausalLM

from .config import OmniModelConfig
from .entrypoints.async_omni import AsyncOmni

# Main entry points
from .entrypoints.omni import Omni

from .version import __version__, __version_tuple__  # isort:skip


# æ³¨å†Œç»™ Transformers
AutoConfig.register("bagel", BagelConfig)
# =========================================================
# æ‹¦æˆª vLLM Configï¼Œä¿®å¤ Worker é‡Œçš„å­—å…¸é—®é¢˜
# =========================================================

_original_with_hf_config = vllm.config.VllmConfig.with_hf_config


def _patched_with_hf_config(self, hf_config, *args, **kwargs):
    """
    æ‹¦æˆªå‡½æ•°ï¼šå¦‚æœ hf_config æ˜¯å­—å…¸ï¼ˆå‘ç”Ÿåœ¨ Worker è¿›ç¨‹ä¸­ï¼‰ï¼Œ
    å¼ºè¡Œå°†å…¶è½¬æ¢å› BagelConfig å¯¹è±¡ã€‚
    """
    # æ£€æŸ¥æ˜¯ä¸æ˜¯å­—å…¸ (è¿™å°±æ˜¯ä½ æŠ¥é”™çš„åŸå› )
    if isinstance(hf_config, dict):
        try:
            # å°è¯•ç”¨æˆ‘ä»¬å®šä¹‰çš„ç±»é‡æ–°å®ä¾‹åŒ–å®ƒ
            # ä½¿ç”¨ **hf_config å°†å­—å…¸è§£åŒ…ä¸ºå‚æ•°
            hf_config = BagelConfig(**hf_config)
        except Exception:
            # å¦‚æœå®ä¾‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨é€šç”¨ Qwen2Config å…œåº•
            hf_config = Qwen2Config(**hf_config)

    # å†æ¬¡æ£€æŸ¥ text_config (é˜²æ­¢ä¹‹å‰çš„ get_text_config æŠ¥é”™)
    if not hasattr(hf_config, "get_text_config"):
        # åŠ¨æ€ç»™å¯¹è±¡ç»‘å®šä¸€ä¸ªæ–¹æ³•
        hf_config.get_text_config = lambda: hf_config

    # è°ƒç”¨ vLLM åŸæ¥çš„é€»è¾‘ï¼Œè¿™æ—¶å€™ hf_config å·²ç»æ˜¯å¯¹è±¡äº†ï¼Œä¸ä¼šæŠ¥é”™
    return _original_with_hf_config(self, hf_config, *args, **kwargs)


# è¦†ç›– vLLM ç±»çš„æ–¹æ³•
vllm.config.VllmConfig.with_hf_config = _patched_with_hf_config

# =========================================================
#  æ³¨å†Œ vLLM æ¨¡å‹å®ç°
# =========================================================


ModelRegistry.register_model("BagelForConditionalGeneration", BagelForConditionalGeneration)
ModelRegistry.register_model("Qwen2ForCausalLM", Qwen2ForCausalLM)

print(" vLLM-Omni åˆå§‹åŒ–å®Œæˆ")

try:
    from . import patch  # noqa: F401
except ModuleNotFoundError as exc:  # pragma: no cover - optional dependency
    if exc.name != "vllm":
        raise
    # Allow importing vllm_omni without vllm (e.g., documentation builds)
    patch = None  # type: ignore


__all__ = [
    "__version__",
    "__version_tuple__",
    # Main components
    "Omni",
    "AsyncOmni",
    # Configuration
    "OmniModelConfig",
    # All other components are available through their respective modules
    # processors.*, schedulers.*, executors.*, etc.
]
