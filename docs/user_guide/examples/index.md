# Examples

vLLM-omni's examples are split into two categories:

- If you are using vLLM-omni from within Python code, see the *Offline Inference* section.
- If you are using vLLM-omni from an HTTP application or client, see the *Online Serving* section.

For detailed example documentation, check the [examples directory](https://github.com/vllm-project/vllm-omni/tree/main/examples) in the repository.
